import os
import cv2
import torch
import torch.nn.functional as F
import numpy as np
import baseline_2d_resnets
import baseline_3d_resnets
from torchvision import transforms
import time

# ===== ì„¤ì • =====
VIDEO_PATH = '/workspace/representation-flow-cvpr19/test/test12.mp4'                      # ì¶”ë¡ í•  ë¹„ë””ì˜¤ ê²½ë¡œ
WEIGHTS_PATH = '/workspace/representation-flow-cvpr19/model.pt'     # .pth ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ
NUM_CLASSES = 7                                   # HMDB51 ë“± ì‚¬ìš©í•œ í´ë˜ìŠ¤ ìˆ˜
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
INPUT_SIZE = 200                                   # ë¹„ë””ì˜¤ í”„ë ˆì„ í¬ê¸°
NUM_FRAMES = 16                                    # ì‚¬ìš©í•  ì—°ì† í”„ë ˆì„ ìˆ˜

# ===== ì „ì²˜ë¦¬ í•¨ìˆ˜ =====
def load_and_preprocess_video(video_path, num_frames=16, size=200):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # â­ ëœë¤ ì‹œì‘ í”„ë ˆì„ ê³„ì‚°
    if total_frames <= num_frames:
        start_frame = 0
    else:
        start_frame = np.random.randint(0, total_frames - num_frames)

    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

    frames = []
    while len(frames) < num_frames:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, (size, size))
        frame = frame[:, :, ::-1]  # BGR to RGB
        frames.append(frame)
    cap.release()

    # ë¶€ì¡±í•œ í”„ë ˆì„ì€ ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì œ
    while len(frames) < num_frames:
        frames.append(frames[-1])

    frames = np.stack(frames, axis=0)  # (T, H, W, C)
    frames = frames.astype(np.float32) / 255.0
    frames = frames.transpose(3, 0, 1, 2)  # -> (C, T, H, W)
    return torch.tensor(frames).unsqueeze(0)  # -> (1, C, T, H, W)

# ===== ëª¨ë¸ ì •ì˜ ë° ë¡œë“œ =====
model = baseline_3d_resnets.resnet50(num_classes=NUM_CLASSES)
state_dict = torch.load(WEIGHTS_PATH, map_location=DEVICE)

if 'state_dict' in state_dict:
    state_dict = state_dict['state_dict']

new_state_dict = {}
for k, v in state_dict.items():
    if k.startswith('module.'):
        new_state_dict[k[7:]] = v
    else:
        new_state_dict[k] = v
model.load_state_dict(new_state_dict, strict=True)  # âœ… strict=True ëª…ì‹œ
model = model.to(DEVICE)
model.eval()

# ===== ë¹„ë””ì˜¤ ë¶ˆëŸ¬ì™€ì„œ ì¶”ë¡  + ì‹œê°„ ì¸¡ì • =====
video_tensor = load_and_preprocess_video(VIDEO_PATH, NUM_FRAMES, INPUT_SIZE)
video_tensor = video_tensor.to(DEVICE)

start_time = time.time()
with torch.no_grad():
    outputs = model(video_tensor)
    probs = F.softmax(outputs, dim=1).squeeze()
    pred_class = torch.argmax(probs).item()
end_time = time.time()

# ===== ê²°ê³¼ ì¶œë ¥ =====
print(f"âœ… Predicted class index: {pred_class}")
print("ğŸ” Class probabilities:")
for i, prob in enumerate(probs.tolist()):
    print(f"  Class {i}: {prob:.4f}")
print(f"â±ï¸ Inference time: {end_time - start_time:.3f} seconds")
